from collections import defaultdict
import numpy as np
import torch
from torchvision import datasets, transforms
from torch.utils.data import Dataset, DataLoader

# Define dataset path
dataset_path = "C:\\Users\\lenovo\\Downloads\\data"

# Define transformations
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

# Load dataset
full_dataset = datasets.ImageFolder(root=dataset_path)

# Organize data by class
class_indices = defaultdict(list)
for idx, (_, label) in enumerate(full_dataset.samples):
    class_indices[label].append(idx)

# Ensure stratified sampling
num_clients = 20
train_ratio = 0.8  # 80% training, 20% testing

client_indices = [[] for _ in range(num_clients)]

# Distribute data across clients while keeping class balance
for label, indices in class_indices.items():
    np.random.shuffle(indices)
    splits = np.array_split(indices, num_clients)
   
    for client_id, split in enumerate(splits):
        client_indices[client_id].extend(split.tolist())

# Ensure exact same number of samples per client
min_train_size = min(len(indices) * train_ratio for indices in client_indices)
min_test_size = min(len(indices) * (1 - train_ratio) for indices in client_indices)

for i in range(num_clients):
    np.random.shuffle(client_indices[i])
    client_indices[i] = client_indices[i][:int(min_train_size + min_test_size)]

# Custom dataset wrapper to apply transformations
class CustomSubset(Dataset):
    def __init__(self, dataset, indices, transform=None):
        self.dataset = dataset
        self.indices = indices
        self.transform = transform
   
    def __getitem__(self, idx):
        image, label = self.dataset[self.indices[idx]]
        if self.transform:
            image = self.transform(image)
        return image, label
   
    def __len__(self):
        return len(self.indices)

# Split each client's data into training and testing sets
train_loaders, test_loaders = [], []
train_sizes, test_sizes = [], []

batch_size = 32
for i, indices in enumerate(client_indices):
    split_idx = int(len(indices) * train_ratio)
   
    train_indices = indices[:split_idx]
    test_indices = indices[split_idx:]

    train_dataset = CustomSubset(full_dataset, train_indices, transform=train_transform)
    test_dataset = CustomSubset(full_dataset, test_indices, transform=test_transform)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)

    train_loaders.append(train_loader)
    test_loaders.append(test_loader)

    train_sizes.append(len(train_dataset))
    test_sizes.append(len(test_dataset))

    print(f"âœ… Client {i+1} Data Loaded: Train={len(train_dataset)}, Test={len(test_dataset)}")
